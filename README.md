
<div align="center">
    <h1><b>Bridging Multiple Modality to LLMs: A Survey on Discrete Tokenization</b></h1>
</div>

The official GitHub page for the survey paper "Bridging Multiple Modality to LLMs: A Survey on Discrete Tokenization".


<div align="center">

![](https://img.shields.io/github/stars/jindongli-Ai/LLM-Discrete-Tokenization-Survey?color=yellow)
![](https://img.shields.io/github/forks/jindongli-Ai/LLM-Discrete-Tokenization-Survey?color=lightblue)
![](https://img.shields.io/github/last-commit/jindongli-Ai/LLM-Discrete-Tokenization-Survey?color=green)
![](https://img.shields.io/badge/PRs-Welcome-blue)
<a href="https://arxiv.org/" target="_blank"><img src="https://img.shields.io/badge/arXiv-xxxx.xxxxx-009688.svg" alt="arXiv"></a>

</div>


<p align="center">
    <img src="./figs/fig_1.png" alt="fig_1" width="460" />
</p>
<br>
<br>

<p align="center">
    <img src="./figs/fig_2.png" alt="fig_2" width="800" />
</p>
<br>
<br>

<p align="center">
    <img src="./figs/fig_12.png" alt="fig_12" width="1000" />
</p>
<br>
<br>


<p align="center">
    <img src="./figs/tab_LLM.png" alt="tab_LLM" width="800" />
</p>
<br>
<br>


<p align="center">
    <img src="./figs/tab_MLLM.png" alt="tab_MLLM" width="800" />
</p>
<br>
<br>








## 2 Fundamental Techniques

### 2.1



### 2.2


### 2.3


### 2.4


### 2.5


### 2.6


### 2.7


### 2.8




## 3 Classical Applications without LLMs

### 3.1



### 3.2


### 3.3


### 3.4


### 3.5


### 3.6


### 3.7


### 3.8

### 3.9


### 3.10


### 3.11


### 3.12



## 4 LLM-based Single-Modality Applications


### 4.1 


### 4.2


### 4.3


### 4.4


### 4.5




## 5 LLM based Multi-Modality Applications

### 5.1 Text + Image

1. 2023_arXiv_SEED_Planting a SEED of Vision in Large Language Model

2. 2024_arXiv_Chameleon_Chameleonï¼šMixed-Modal Early-Fusion Foundation Models

3. 2024_arXiv_ILLUME_ILLUME=Illuminating Your LLMs to See, Draw, and Self-Enhance

4. 2024_arXiv_ILLUME_ILLUME=Illuminating Your LLMs to See, Draw, and Self-Enhance

5. 2024_arXiv_Lumina-mGPT_Lumina-mGPT=Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining

6. 2024_arXiv_MUSE-VL_MUSE-VLï¼šModeling Unified VLM through Semantic Discrete Encoding

7. 2024_arXiv_Show-o_Show-oï¼šOne single transformer to unify multimodal understanding and generation

8. 2024_ICLR_LaVIT_Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization

9. 2024_ICLR_SEED-LLaMA_Making LLaMA SEE and Draw with SEED Tokenizer

10. 2024_ICML_Libra_Libra= Building Decoupled Vision System on Large Language Models

11. 2024_ICML_Morph-Tokens_Auto-Encoding Morph-Tokens for Multimodal LLM

12. 2025_arXiv_DDT-LLaMA_Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens

13. 2025_arXiv_ETT_End-to-End-Vision Tokenizer Tuning

14. 2025_arXiv_FashionM3_FashionM3=Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model

15. 2025_arXiv_HiMTok_HiMTokï¼šLearning Hierarchical Mask Tokens for Image Segmentation with Large Multimodal Model

16. 2025_arXiv_ILLUME+_ILLUME+ï¼šIlluminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement

17. 2025_arXiv_Janus-Pro_Janus-Pro=Unified Multimodal Understanding and Generation with Data and Model Scaling

18. 2025_arXiv_QLIP_QLIPï¼šText-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation

19. 2025_arXiv_SemHiTok_SemHiTokï¼šA Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation

20. 2025_arXiv_UniToken_UniTokenï¼šHarmonizing Multimodal Understanding and Generation through Unified Visual Encoding

21. 2025_arXv_Token-Shuffle_Token-Shuffleï¼šTowards High-Resolution Image Generation with Autoregressive Models

22. 2025_AAAI_MARS_MARS=Mixture of Auto-Regressive Models for Fine-grained Text-to-image Synthesis

23. 2025_CVPR_TokenFlow_TokenFlowï¼šUnified Image Tokenizer for Multimodal Understanding and Generation

24. 2025_ICLR_ClawMachine_ClawMachine=Fetching Visual Tokens as An Entity for Referring and Grounding





### 5.2 Text + Audio

1. 2023_arXiv_AudioPaLM_AudioPaLM=A Large Language Model That Can Speak and Listen

2. 2023_arXiv_LauraGPT_LauraGPT=Listen, Attend, Understand, and Regenerate Audio with GPT

3. 2023_EMNLP_SpeechGPT_SpeechGPT=Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities

4. 2024_arXiv_CosyVoice 2_CosyVoice 2=Scalable Streaming Speech Synthesis with Large Language Models

5. 2024_arXiv_CosyVoice_CosyVoice_A Scalable Multilingual Zero-shot Text-to-Speech Synthesizer based on Supervised Semantic Tokens

6. 2024_arXiv_IntrinsicVoice_IntrinsicVoice=Empowering LLMs with Intrinsic Real-time Voice Interaction Abilities

7. 2024_arXiv_Moshi_Moshiï¼ša speech-text foundation model for real-time dialogue

8. 2024_arXiv_OmniFlatten_OmniFlatten=An End-to-end GPT Model for Seamless Voice Conversation

9. 2024_arXiv_T5-TTS_Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment

10. 2024_ICASSP_VoxtLM_VoxtLM=Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks

11. 2024_IEEE Signal Processing Letters_MSRT_Tuning Large Language Model for Speech Recognition With Mixed-Scale Re-Tokenization

12. 2024_Interspeech_DiscreteSLU_DiscreteSLU= A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding

13. 2024_MM_GPT-Talker_Generative Expressive Conversational Speech Synthesis

14. 2025_arXiv_Spark-TTS_Spark-TTSï¼šAn Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens

15. 2025_Kimi-Audio_Kimi-Audio Technical Report





### 5.3 Text + Video

1. 2024_arXiv_Loong_Loong=Generating Minute-level Long Videos with Autoregressive Language Models

2. 2024_ICML_Video-LaVIT_Video-LaVIT=Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization

3. 2025_arXiv_HiTVideo_HiTVideo=Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models







### 5.4 Text + Graph

1. 2024_arXiv_UniMoT_UniMoT_Unified Molecule-Text Language Model with Discrete Token Representation

2. 2024_ICML Workshop_HIGHT_Improving Graph-Language Alignment with Hierarchical Graph Tokenization

3. 2025_arXiv_MedTok_Multimodal Medical Code Tokenizer

4. 2025_arXiv_SSQR_Self-Supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models






### 5.5 Text + Motion

1. 2024_arXiv_MotionGlot_MotionGlot=A Multi-Embodied Motion Generation Model

2. 2024_CVPR_AvatarGPT_AvatarGPT=All-in-One Framework for Motion Understanding, Planning, Generation and Beyond

3. 2024_ECCV_Semgrasp_Semantic grasp generation via language aligned discretization

4. 2024_IV_Walk-the-Talk_Walk-the-Talk=LLM driven Pedestrian Motion Generation






### 5.6 Text + Image + Audio

1. 2023_arXiv_TEAL_TEAL=Tokenize and Embed ALL for Multi-modal Large Language Models

2. 2024_arXiv_DMLM_Discrete Multimodal Transformers with A Pretrained Large Language Model for Mixed-Supervision Speech Processing

3. 2024_ACL_AnyGPT_AnyGPT= Unified Multimodal LLM with Discrete Sequence Modeling






### 5.7 Text + Image + Video

1. 2024_arXiv_Emu3ï¼šNext-Token Prediction is All You Need

2. 2025_ICLR_LWM_World model on million-length video and language with blockwise ringattention

3. 2025_ICLR_VILA-U_VILA-U= A Unified Foundation Model Integrating Visual Understanding and Generation







### 5.8 Text + Audio + Motion

1. 2025_ICCGV_LLM Gesticulator_LLM Gesticulator=Levaraging Large Language Models for Scalable and Controllable Co-Speech Gesture Synthesis







### 5.9 Text + Image + Audio + Action

1. 2024_CVPR_Unified-IO 2_Unified-IO 2ï¼šScaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action





### 5.10 Text + Image + Video + Audio

1. 2024_arXiv_MIO_MIO=A foundation model on multimodal tokens

2. 2024_ICML_VideoPoet_VideoPoet= A Large Language Model for Zero-Shot Video Generation






## 6 Challenges and Future Directions



## Related Survey


### non-LLM

### LLM


## Related Reposority


## ðŸ“– Citation







